æˆ‘ä»¬ä¼šæµ‹è¯•ä»¥ä¸‹å››ç§æ–¹æ³•ï¼š

    æ­£åˆ™è¡¨è¾¾å¼ï¼ˆRegexï¼‰ â€”â€” ä¼ ç»Ÿæ–¹æ³•
    pkusegï¼ˆåŒ—å¤§åˆ†è¯ï¼‰ â€”â€” æœºå™¨å­¦ä¹ ä¼˜åŒ–
    BERT åˆ†å¥ï¼ˆbert-base-chineseï¼‰ â€”â€” æ·±åº¦å­¦ä¹ æ–¹æ³•
    Ziya-LLaMA-13B â€”â€” è¶…å¤§æ¨¡å‹ï¼Œæœ€å…ˆè¿›

ğŸš€ æµ‹è¯•ç¯å¢ƒ

    GPU: RTX 3090 (24GB)
    CPU: Intel i9-12900Kï¼ˆæˆ–ç±»ä¼¼é«˜æ€§èƒ½ CPUï¼‰
    æ–‡æœ¬è§„æ¨¡: 100 ä¸‡å­—ï¼ˆçº¦ 5000-7000 å¥ï¼‰
    ç›®æ ‡: è®¡ç®—æ¯ç§æ–¹æ³•çš„è¿è¡Œæ—¶é—´

âœ… ä»£ç å®ç°

æˆ‘ä»¬å°†ç”¨ Python è¿›è¡Œæµ‹è¯•ï¼Œå¹¶ä½¿ç”¨ time è®¡ç®—å¤„ç†é€Ÿåº¦ã€‚
## 1ï¸âƒ£ æ­£åˆ™è¡¨è¾¾å¼ï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰

```python
import re
import time

def split_by_regex(text):
    start_time = time.time()
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', text)
    sentences = ["".join(sentences[i:i+2]).strip() for i in range(0, len(sentences)-1, 2)]
    sentences = [s for s in sentences if s]
    end_time = time.time()
    print(f"æ­£åˆ™è¡¨è¾¾å¼åˆ†å¥å®Œæˆï¼Œè€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
    return sentences
```
# è¯»å–å¤§æ–‡æœ¬
with open("novel.txt", "r", encoding="utf-8") as f:
    text = f.read()

# è¿è¡Œæµ‹è¯•
regex_sentences = split_by_regex(text)

âœ… é¢„è®¡è¿è¡Œæ—¶é—´ï¼ˆ100 ä¸‡å­—ï¼‰ï¼š

    0.5 - 1 ç§’ï¼ˆæœ€å¿«ï¼‰

ğŸ“Œ ç¼ºç‚¹ï¼š

    è¯¯åˆ†å‰²è¾ƒå¤šï¼Œå®¹æ˜“åœ¨å¼•å·ã€ç‰¹æ®Šç¬¦å·å¤„å‡ºé”™ã€‚


## 2ï¸âƒ£ pkusegï¼ˆæœºå™¨å­¦ä¹ æ–¹æ³•ï¼‰
```python
import pkuseg
import time

def split_by_pkuseg(text):
    start_time = time.time()
    seg = pkuseg.pkuseg()
    words = seg.cut(text)
    sentences = []
    temp = ""
    for word in words:
        temp += word
        if word in "ã€‚ï¼ï¼Ÿ":
            sentences.append(temp)
            temp = ""
    if temp:
        sentences.append(temp)
    end_time = time.time()
    print(f"pkuseg åˆ†å¥å®Œæˆï¼Œè€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
    return sentences
```
# è¿è¡Œæµ‹è¯•
pkuseg_sentences = split_by_pkuseg(text)

âœ… é¢„è®¡è¿è¡Œæ—¶é—´ï¼ˆ100 ä¸‡å­—ï¼‰ï¼š

    5 - 15 ç§’ï¼ˆæ¯”æ­£åˆ™æ…¢ï¼Œä½†æ›´å‡†ç¡®ï¼‰

ğŸ“Œ ä¼˜ç‚¹ï¼š

    æ¯”æ­£åˆ™æ›´æ™ºèƒ½ï¼Œèƒ½è¯†åˆ«ç‰¹æ®Šç»“æ„çš„å¥å­ã€‚

ğŸ“Œ ç¼ºç‚¹ï¼š

    è¾ƒæ…¢ï¼Œå¯¹äºè¶…å¤§æ–‡æœ¬ä»ç„¶æœ‰æ€§èƒ½ç“¶é¢ˆã€‚

## 3ï¸âƒ£ BERT åˆ†å¥ï¼ˆbert-base-chineseï¼Œæ·±åº¦å­¦ä¹ ï¼‰

from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import time

# åŠ è½½ BERT ä¸­æ–‡åˆ†å¥æ¨¡å‹
model_name = "ckiplab/bert-base-chinese-ws"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)
model.to("cuda")  # ä½¿ç”¨ GPU åŠ é€Ÿ

def split_by_bert(text):
    start_time = time.time()
    
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    inputs = {key: val.to("cuda") for key, val in inputs.items()}  # ä¼ å…¥ GPU
    
    with torch.no_grad():
        outputs = model(**inputs).logits

    predictions = torch.argmax(outputs, dim=-1).squeeze().tolist()
    
    sentences = []
    temp = ""
    tokens = tokenizer.tokenize(text)
    
    for token, pred in zip(tokens, predictions):
        temp += token.replace("##", "")
        if pred == 1:  # é¢„æµ‹ä¸ºå¥å­ç»“æŸ
            sentences.append(temp)
            temp = ""

    if temp:
        sentences.append(temp)
    
    end_time = time.time()
    print(f"BERT åˆ†å¥å®Œæˆï¼Œè€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
    return sentences

# è¿è¡Œæµ‹è¯•
bert_sentences = split_by_bert(text)

âœ… é¢„è®¡è¿è¡Œæ—¶é—´ï¼ˆ100 ä¸‡å­—ï¼‰ï¼š

    30 - 90 ç§’ï¼ˆGPU åŠ é€Ÿï¼‰

ğŸ“Œ ä¼˜ç‚¹ï¼š

    æ¯” pkuseg æ›´å‡†ç¡®ï¼Œèƒ½ç†è§£è¯­å¢ƒã€‚
    ä¸ä¼šè¯¯åˆ†å‰²ï¼Œé€‚ç”¨äºå°è¯´ã€å¯¹è¯ç­‰å¤æ‚æ–‡æœ¬ã€‚

ğŸ“Œ ç¼ºç‚¹ï¼š

    å¤„ç†é•¿æ–‡æœ¬æ—¶å¯èƒ½éœ€è¦åˆ†å—ï¼ˆBERT æœ€å¤§è¾“å…¥ 512 tokensï¼‰ã€‚
    æ¯”æ­£åˆ™å’Œ pkuseg æ…¢ï¼Œä½†æ›´æ™ºèƒ½ã€‚

## 4ï¸âƒ£ LLaMA-13B åˆ†å¥ï¼ˆæœ€å…ˆè¿›æ–¹æ³•ï¼‰

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import time

# åŠ è½½ LLaMA ä¸­æ–‡æ¨¡å‹
model_name = "IDEA-CCNL/Ziya-LLaMA-13B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
model.to("cuda")

def split_by_llama(text):
    start_time = time.time()
    
    # LLaMA éœ€è¦æ›´å¤§è¾“å…¥é•¿åº¦ï¼Œé€‚ç”¨äºé•¿æ–‡æœ¬
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {key: val.to("cuda") for key, val in inputs.items()}
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=4096)

    sentences = tokenizer.decode(outputs[0], skip_special_tokens=True).split("\n")
    end_time = time.time()
    print(f"LLaMA åˆ†å¥å®Œæˆï¼Œè€—æ—¶ï¼š{end_time - start_time:.2f} ç§’")
    return [s.strip() for s in sentences if s]

# è¿è¡Œæµ‹è¯•
llama_sentences = split_by_llama(text)

âœ… é¢„è®¡è¿è¡Œæ—¶é—´ï¼ˆ100 ä¸‡å­—ï¼‰ï¼š

    180 - 600 ç§’ï¼ˆä¾èµ– GPUï¼‰

ğŸ“Œ ä¼˜ç‚¹ï¼š

    æœ€æ™ºèƒ½ï¼Œç†è§£è¯­å¢ƒã€ä¸ä¼šè¯¯åˆ†å‰²ã€‚
    é€‚ç”¨äºè¶…é•¿æ–‡æœ¬ï¼ˆå°è¯´ã€æ³•å¾‹æ–‡æ¡£ï¼‰ã€‚

ğŸ“Œ ç¼ºç‚¹ï¼š

    è¶…çº§æ…¢ï¼Œæ¯” BERT æ…¢ 3-10 å€ã€‚
    éœ€è¦é«˜ç«¯ GPUï¼ˆA100 ä»¥ä¸Šæ›´åˆé€‚ï¼‰ã€‚

ğŸ† ç»“æœå¯¹æ¯”ï¼šå¤„ç† 100 ä¸‡å­— txt æ–‡ä»¶
æ–¹æ³•	é€Ÿåº¦ï¼ˆ100 ä¸‡å­—ï¼‰	æ˜¯å¦é€‚åˆå°è¯´	è¯¯åˆ‡å‰²ç‡	é€‚ç”¨åœºæ™¯
æ­£åˆ™è¡¨è¾¾å¼	0.5 - 1 ç§’	âŒ ä¸€èˆ¬	é«˜	ä»…é€‚ç”¨äºç®€å•æ–‡æœ¬
pkuseg	5 - 15 ç§’	âœ… å¯ç”¨	ä¸­	æ–°é—»ã€æ™®é€šæ–‡æœ¬
BERT	30 - 90 ç§’	âœ… å¾ˆå¥½	ä½	å¤æ‚æ–‡æœ¬ï¼ˆå°è¯´ã€å¯¹è¯ï¼‰
LLaMA-13B	180 - 600 ç§’	âœ… æœ€ä¼˜	æä½	é«˜ç«¯ç ”ç©¶ã€è¶…é•¿æ–‡æœ¬
ğŸš€ ä½ çš„ RTX 3090 24GBï¼Œæ¨è

âœ… æœ€å¿«æ–¹æ¡ˆï¼ˆå¦‚æœè¦é€Ÿåº¦ï¼‰ï¼špkuseg
âœ… å¹³è¡¡æ–¹æ¡ˆï¼ˆå¦‚æœè¦ç²¾åº¦ï¼‰ï¼šbert-base-chinese
âœ… æœ€å¼ºæ–¹æ¡ˆï¼ˆå¦‚æœè¦æœ€å…ˆè¿› AIï¼‰ï¼šZiya-LLaMA-13B

ä½ è¦æˆ‘å¸®ä½ ä¼˜åŒ–å“ªç§æ–¹æ¡ˆï¼ŸğŸ”¥
